{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "elementary-marina",
   "metadata": {},
   "source": [
    "# Lab 9: Natural Language Processing\n",
    "COSC 410: Applied Machine Learning\\\n",
    "Colgate University\\\n",
    "*Prof. Apthorpe*\n",
    "\n",
    "This lab is due to Gradescope by the beginning of lab next week (2:45p on 4/7). You may work with a partner on this lab – if you do, submit only one solution as a “group” on Gradescope. \n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, you will implement a recurrent neural network to perform text generation.  The network you will create will perform **character-level forecasting**. Given a sequence of characters, the model will predict the next character in the sequence. When applied iteratively, this allows the model to generate new sequences of text. Note that the model will never be given specific instruction about English spelling, grammar, or other conventions. It will try to learn all of these things from the training input. \n",
    "\n",
    "We will be using plain text files as training data, starting with the Brothers Grimm fairytale \"Little Red-Cap\" (known in America as \"Little Red Riding Hood\").  This text is on the short end of the amount of training input needed to train a text generation model and may result in generated text that mimics entire passages of the input. However, a smaller input text dramatically reduces training time while still showing how the process works -- perfect for this lab exercise.\n",
    "\n",
    "## Provided Files\n",
    " * `Lab9.ipynb`: This file\n",
    " * `red_riding_hood.txt`: plaintext version of the Brothers Grimm fairytale \"Little Red-Cap\" \n",
    " \n",
    "## Part 1: Data Import and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "blind-wrist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as ks\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-astronomy",
   "metadata": {},
   "source": [
    "Complete the `load_input` function, which should \n",
    "  1) load a `.txt` file into one (long) string\n",
    "  2) replace all '\\n' characters with ' ' (space) characters\n",
    "  3) convert all characters to lowercase\n",
    "  4) return the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "agricultural-bachelor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_input(filename):\n",
    "    with open('red_riding_hood.txt', 'r') as file:\n",
    "        data = file.read().replace('\\n', ' ').lower()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-leonard",
   "metadata": {},
   "source": [
    "RNNs can't operate on strings directly, so we need to convert the characters into integers.\n",
    "\n",
    "Complete the following functions to compute the **vocabulary** of the text (a list containing all the **unique** characters in the text), encode string texts into integer lists, and decode integer lists back to string texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "failing-satellite",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(text):\n",
    "    \"\"\"Returns a list of all UNIQUE characters in string text in any arbitrary order. \n",
    "    \n",
    "       Example:\n",
    "          vocab(\"Hello, World\") --> [\"H\", \"e\", \"l\", \"o\", \",\", \" \", \"W\", \"r\", \"d\"]\n",
    "       \n",
    "       The reference implementation is 2 LoC using the set() and list() functions\"\"\"\n",
    "    return list(set(text))\n",
    "\n",
    "def encode(text, vocab):\n",
    "    \"\"\"Converts string text into a list of integers, with one integer per character. The \n",
    "       integers should correspond to that character's index in the vocab list\n",
    "    \n",
    "       Example:\n",
    "          vocab = get_vocab(\"Hello, World\")\n",
    "          encode(\"Hello, World\", vocab) --> [0, 1, 2, 2, 3, 4, 5, 6, 3, 7, 2, 8]\n",
    "       \n",
    "       The reference implementation is 3 LoC.\"\"\"\n",
    "    ints = []\n",
    "    \n",
    "    for x in text:\n",
    "        ints.append(vocab.index(x))\n",
    "    return ints\n",
    "\n",
    "def decode(tokens, vocab):\n",
    "    \"\"\"Converts tokens (list of integers) back into string text. The \n",
    "       integers should correspond to each character's index in the vocab list\n",
    "    \n",
    "       Example:\n",
    "          vocab = get_vocab(\"Hello, World\")\n",
    "          decode([0, 1, 2, 2, 3, 4, 5, 6, 3, 7, 2, 8], vocab) --> \"Hello, World\"\n",
    "       \n",
    "       The reference implementation is 4 LoC.\"\"\"\n",
    "    text = \"\"\n",
    "    for x in tokens:\n",
    "        text += vocab[x]\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-continuity",
   "metadata": {},
   "source": [
    "Next we need to create training examples and training labels for our model. The goal of the model is to take a sequence of characters and predict what character should come next. Complete the following function to divide the text into overlapping *subsequences* of characters (training examples) and a list of the characters immediately after each subsequence (training labels). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "optical-shark",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(tokens, seq_length):\n",
    "    \"\"\"Divides tokens (list of integers) into overlapping subsequences of length seq_length.\n",
    "       Returns these subsequences as a list of lists, also returns a list with the \n",
    "       integer value immediately following each subsequence\n",
    "    \n",
    "       Example:\n",
    "          generate_sequences([0, 1, 2, 2, 3, 4, 5, 6, 3, 7, 2, 8], 4) -->\n",
    "              [[0, 1, 2, 2],\n",
    "               [1, 2, 2, 3],\n",
    "               [2, 2, 3, 4],\n",
    "               [2, 3, 4, 5],\n",
    "               [3, 4, 5, 6],\n",
    "               [4, 5, 6, 3],\n",
    "               [5, 6, 3, 7], \n",
    "               [6, 3, 7, 2]]]  (1st return value)\n",
    "               \n",
    "             [3, 4, 5, 6, 3, 7, 2, 8]  (2nd return value)\n",
    "       \n",
    "       The reference implementation is 6 LoC.\"\"\"\n",
    "    seqs = []\n",
    "    breaks = []\n",
    "    x = 0\n",
    "    while x + seq_length < len(tokens):\n",
    "        seqs.append(tokens[x:x+seq_length])\n",
    "        x += 1\n",
    "        breaks.append(tokens[x])\n",
    "    return seqs, breaks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-configuration",
   "metadata": {},
   "source": [
    "If you have programmed the previous functions correctly, the following cell will run with no errors and produce the following output:\n",
    "```\n",
    "Length of input text (in characters): 7376\n",
    "Vocab size: 36\n",
    "Training examples shape: (7325, 50)\n",
    "Training labels shape: (7325,)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "prescription-fellowship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input text (in characters): 7376\n",
      "Vocab size: 36\n",
      "Training examples shape: (7326, 50)\n",
      "Training labels shape: (7326,)\n"
     ]
    }
   ],
   "source": [
    "text = load_input(\"red_riding_hood.txt\")\n",
    "\n",
    "vocab = get_vocab(text)\n",
    "tokens = encode(text, vocab)\n",
    "assert(decode(tokens, vocab) == text)\n",
    "\n",
    "seq_length = 50\n",
    "\n",
    "x, y = generate_sequences(tokens, seq_length)\n",
    "x, y = np.array(x), np.array(y)\n",
    "\n",
    "print(f\"Length of input text (in characters): {len(text)}\")\n",
    "print(f\"Vocab size: {len(vocab)}\")\n",
    "print(f\"Training examples shape: {x.shape}\")\n",
    "print(f\"Training labels shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-wheat",
   "metadata": {},
   "source": [
    "## Part 2: RNN Creation & Training\n",
    "\n",
    "Complete the following function that creates and compiles an LSTM model for character prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "prime-march",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, embedding_dim, rnn_units):\n",
    "    \"\"\"Creates, compiles, and returns a LSTM model for character prediction. The model should have \n",
    "       at least 3 layers: an Embedding layer, a LSTM layer, and a Dense layer. \n",
    "       The model should produce 1 prediction per input sequence (i.e. the next character following the sequence),\n",
    "       NOT 1 prediction per step of the sequence.\n",
    "       \n",
    "       Arguments:\n",
    "          vocab_size: number of unique characters accross all training examples, also the input size of the Embedding layer\n",
    "          embedding_dim: output size of Embedding layer\n",
    "          rnn_units: number of units in LSTM layer\n",
    "          \n",
    "       Use the \"adam\" optimizer for best performance.\n",
    "       \n",
    "       The reference implementation is 7 LoC using the Keras Sequential API\n",
    "    \"\"\"\n",
    "    model = ks.Sequential([\n",
    "    ks.layers.Input(shape=(None,)),\n",
    "    ks.layers.Embedding(vocab_size,embedding_dim),\n",
    "    ks.layers.LSTM(rnn_units, activation='sigmoid', return_sequences = False),\n",
    "    ks.layers.Dense(vocab_size, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-hacker",
   "metadata": {},
   "source": [
    "Complete the following function that takes a trained model and uses it to generate new text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "written-ending",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, seed, num_chars, vocab):\n",
    "    \"\"\"Iteratively runs model.predict() to generate successive characters starting from the characters in seed. \n",
    "       Each generated character is appended to the input of the following model.predict() call. \n",
    "       \n",
    "       Returns the generated text decoded back into a string.\n",
    "       \n",
    "       Remember that model.predict will return a probability distribution, not a single integer. \n",
    "       You will need to convert these probabilities into an integer by RANDOMLY SAMPLING an index\n",
    "       based on the distribution weights, NOT by using np.argmax (which can lead to repetitions in generated text)\n",
    "       \n",
    "       You will have to be careful with your array shapes. You will want to include print statements to inspect\n",
    "           the shapes of intermediate values to help with debugging.\n",
    "       \n",
    "       Arguments:\n",
    "          model: trained model\n",
    "          seed: string with \"starter\" seed for text generation. This will need to be encoded before it is used in model.predict\n",
    "          num_chars: the number of characters that should be generated\n",
    "          vocab: list of unique characters in all training examples\n",
    "       \n",
    "       The reference implementation is 7 LoC\n",
    "    \"\"\"\n",
    "    \n",
    "    vdf = pd.DataFrame(vocab)\n",
    "    for x in range(num_chars):\n",
    "        \n",
    "        tseed = encode(seed,vocab)\n",
    "        t = model.predict([tseed])\n",
    "        nextchar = vdf.sample(n=1,weights=t[0],axis=0)\n",
    "        n = str(nextchar)\n",
    "        \n",
    "        seed += n[-1]\n",
    "        \n",
    "    \n",
    "    return seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-bookmark",
   "metadata": {},
   "source": [
    "To test the `create_model` and `generate_text` functions, the following cell creates a model and uses it to generate 10 characters *untrained*. This will produce gibberish, but will let you know whether there are runtime errors you need to fix before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "dental-admission",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a“’vqruc.ymfbwtks ug \n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 256\n",
    "rnn_units = 512\n",
    "seed = \"a\"\n",
    "num_chars_to_generate = 20\n",
    "\n",
    "model = create_model(len(vocab), embedding_dim, rnn_units)\n",
    "\n",
    "generated_text = generate_text(model, seed, num_chars_to_generate, vocab)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-outside",
   "metadata": {},
   "source": [
    "Once you have the previous cell working, it is time to train! The following two cells create and train a model, printing some example generated text after each epoch. You can stop and resume the training at any point by interrupting the kernel and then re-running the cell that calls `model.fit`. As the training progresses, you will hopefully see the generated text looking more and more like English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "growing-volunteer",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "rnn_units = 512\n",
    "batch_size = 128\n",
    "epochs = 30\n",
    "seed = \"a\"\n",
    "num_chars_to_generate = 100\n",
    "\n",
    "generate_text_callback = ks.callbacks.LambdaCallback(on_epoch_end=lambda epoch, log: print(generate_text(model, seed, num_chars_to_generate, vocab)))\n",
    "model = create_model(len(vocab), embedding_dim, rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "weighted-equity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.9888 - accuracy: 0.1750af’t  ueedescre t:weh  d sd  s. a gepw.suoi  rnihmd ,rarrssha to iso yh  arg  l hh’hyoeeao nd cr,gdoo\n",
      "58/58 [==============================] - 48s 806ms/step - loss: 2.9888 - accuracy: 0.1750\n",
      "Epoch 2/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.9444 - accuracy: 0.1867al’iae w l e tfeeiread p osaeteda e .yestgwoma wf tea ldo  e.s  ato-aete ’tdt d’mee;ed;rdlt ar perhhp\n",
      "58/58 [==============================] - 43s 751ms/step - loss: 2.9444 - accuracy: 0.1867\n",
      "Epoch 3/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.9464 - accuracy: 0.1911asle aadeieiw d l i auo  ,toee-ranteeooon  u;l trv li eotc,awh  pcw dtaalfa  yhe hwaa fof apgsdea drs\n",
      "58/58 [==============================] - 44s 754ms/step - loss: 2.9464 - accuracy: 0.1911\n",
      "Epoch 4/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.9353 - accuracy: 0.1911aovea kp .tuh,ilitsl eeugebrio,pave rdwoitna uoodoo paese tateo r, b  ttwpartoehleewg ;s,kdlts .the o\n",
      "58/58 [==============================] - 43s 747ms/step - loss: 2.9353 - accuracy: 0.1911\n",
      "Epoch 5/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.9278 - accuracy: 0.1896ahcoecls tpnrtet fwfnooeeg ,eeeed.ifhm  hf rdhae’athor lwhiahwalrgi r srgawerinrh on rsn- rrtdn  a dr\n",
      "58/58 [==============================] - 44s 760ms/step - loss: 2.9278 - accuracy: 0.1896\n",
      "Epoch 6/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.9240 - accuracy: 0.1904ash eretateehrhrce p oodhhatelmuhseloieiur  ge inwoac h y lha o’im,dt hdac,nty eh htue.o os te htha  \n",
      "58/58 [==============================] - 44s 755ms/step - loss: 2.9240 - accuracy: 0.1904\n",
      "Epoch 7/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.9251 - accuracy: 0.1911abaaiuin  ‘n aoa’ dtobrae  ohulnee en  cs e dd o eeuspan edd  nrrwortdo itnweedoeoe  otntneh nsseecs!\n",
      "58/58 [==============================] - 43s 742ms/step - loss: 2.9251 - accuracy: 0.1911\n",
      "Epoch 8/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.9192 - accuracy: 0.1916aonk ed seosaesa raw es  qneooi dsteosel ssedbutd c,- i i‘dh  ’eoheeoa at’s bfle drd  b,an ! rf-at   \n",
      "58/58 [==============================] - 43s 744ms/step - loss: 2.9192 - accuracy: 0.1916\n",
      "Epoch 9/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.9120 - accuracy: 0.1910am tt wrot‘a whrsrknoe ws , oed e hrpk se d’hsf,rvmcsm t tna whsdgdh rdud, va  hsh trlal ui arngnwoo \n",
      "58/58 [==============================] - 44s 759ms/step - loss: 2.9120 - accuracy: 0.1910\n",
      "Epoch 10/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.9122 - accuracy: 0.1897awhn thwlkn  ti srwtheaneghe .c tty“soknhui’ra i .e i.amt, eeea sr tnw  te,’dsetoi brdt.rpmsfeho taw \n",
      "58/58 [==============================] - 44s 755ms/step - loss: 2.9122 - accuracy: 0.1897\n",
      "Epoch 11/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.9060 - accuracy: 0.1911adbyhlitel ryrmcigulwrihsfr,uy d d awrrteo  w frc yh oeeahty a hehnda , eneg diehaon ekdtn spe  grean\n",
      "58/58 [==============================] - 43s 744ms/step - loss: 2.9060 - accuracy: 0.1911\n",
      "Epoch 12/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.9034 - accuracy: 0.1915a  licaetaynrltt hehl, rh n, kcneluarhesdta t.c anient  aheephstmhs‘ehgdeari o ehd,ottuueddogh, lo ep\n",
      "58/58 [==============================] - 44s 754ms/step - loss: 2.9034 - accuracy: 0.1915\n",
      "Epoch 13/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.9015 - accuracy: 0.1906alm,d dh  ftrfcnle lg   ra .dssi astldd oh g  naferulg e iavehrre a hat gemhrseog  snwlhtowht dohlais\n",
      "58/58 [==============================] - 61s 1s/step - loss: 2.9015 - accuracy: 0.1906\n",
      "Epoch 14/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.8958 - accuracy: 0.1901armh hihctve;n   mymetftlhaahet is ti dn aeti‘blaeosu‘tht o tdho eue,e nesb,egicsa  tetnyatnlht:!see \n",
      "58/58 [==============================] - 52s 901ms/step - loss: 2.8958 - accuracy: 0.1901\n",
      "Epoch 15/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.8946 - accuracy: 0.1901arm .tsru a eyda -wrsooraea mhrenrtdgio ye eerasd  aset,o wae tooee’e irtetprhodgnoh, efn ieogseeitod\n",
      "58/58 [==============================] - 60s 1s/step - loss: 2.8946 - accuracy: 0.1901\n",
      "Epoch 16/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.8937 - accuracy: 0.1884ahetbr  ep h  awo besoeeg ri sowoo t’adeota mal od   !e brh ne   owhp hra ywailn agetad ea hf  hs, h,\n",
      "58/58 [==============================] - 48s 834ms/step - loss: 2.8937 - accuracy: 0.1884\n",
      "Epoch 17/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.8896 - accuracy: 0.1915ab  h aw  au t oh yew swhenaf v,dwl d te ws ytya’ham dour ,ea ‘rhtehnc;a hon lgttrb   rr r’attrt  yes\n",
      "58/58 [==============================] - 47s 820ms/step - loss: 2.8896 - accuracy: 0.1915\n",
      "Epoch 18/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.8845 - accuracy: 0.1915ailav  sia eleturic   w,:tfe u    i l et:tawo  eude n ma  oo afen lniosdtennytrnnos tt io eeehdfee st\n",
      "58/58 [==============================] - 47s 810ms/step - loss: 2.8845 - accuracy: 0.1915\n",
      "Epoch 19/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.8829 - accuracy: 0.1896ablhat ! turftranwdtosehocnter og tdtenweroa aewg saowwn ’p rbtfo fea oeesr:sor! aiohtgorh hddooe dt \n",
      "58/58 [==============================] - 50s 858ms/step - loss: 2.8829 - accuracy: 0.1896\n",
      "Epoch 20/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.8800 - accuracy: 0.1906a ”tdohese on .r  edh nneh i ipea da tc,asagraurthnoreeh wsed lc,,ouru   ydhoinri sda eda atdtat wtri\n",
      "58/58 [==============================] - 52s 889ms/step - loss: 2.8800 - accuracy: 0.1906\n",
      "Epoch 21/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.8758 - accuracy: 0.1911acigeetd wea  yo  ihif‘enw,n    t rdywenoedhy.b awoeh  ta t,lt greph eewd bo  awhl.bh,ed onelfcy  eni\n",
      "58/58 [==============================] - 52s 901ms/step - loss: 2.8758 - accuracy: 0.1911\n",
      "Epoch 22/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.8746 - accuracy: 0.1919a d e na’n se  intt,arta’h.t rl;st,do ild q rga’hm,r m  ‘grd  w aweea ak   tlas trneo ti.osrerkdtimeh\n",
      "58/58 [==============================] - 48s 822ms/step - loss: 2.8746 - accuracy: 0.1919\n",
      "Epoch 23/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.8718 - accuracy: 0.1927aoult anat r ue oe   lwe  ohl ‘ doo  oc.t wy gttt a ksa ve pruga  ,  goe lh ‘eed  ro a otag olwt s e \n",
      "58/58 [==============================] - 48s 827ms/step - loss: 2.8718 - accuracy: 0.1927\n",
      "Epoch 24/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.8638 - accuracy: 0.1919a nibnghlrotrraehhinhvnefehc eukto ,a r  t j h shrena nnooe ki i ‘‘emtht,adusoieire  rfks”hslte itroo\n",
      "58/58 [==============================] - 48s 838ms/step - loss: 2.8638 - accuracy: 0.1919\n",
      "Epoch 25/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.8592 - accuracy: 0.1934akghsone dtetsssodouorcnrrddg,ad ! b  nrhogtodsltleereerehdcool och apdduttd afeeen,a rb  hvhsavnuset\n",
      "58/58 [==============================] - 49s 851ms/step - loss: 2.8592 - accuracy: 0.1934\n",
      "Epoch 26/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.8560 - accuracy: 0.1933a :ib rhssf a eh inpbgedi’oh;bra e t oowteil llh w etoa uirse n ot oa-hgwsrr ,dedfhekoa: e u t a brt \n",
      "58/58 [==============================] - 49s 844ms/step - loss: 2.8560 - accuracy: 0.1933\n",
      "Epoch 27/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.8511 - accuracy: 0.1929a   eastiglt’hr ts o  lethhh: ftdorotalneeaeey   a esna te   r: wm; byotehy as;oo tost tsotc ere me s\n",
      "58/58 [==============================] - 48s 827ms/step - loss: 2.8511 - accuracy: 0.1929\n",
      "Epoch 28/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.8451 - accuracy: 0.1930a r hns l,pdtta lth er iwp o  p   ed  tn hdnetnnytootmhocsl f d i oogchttoooe,siftlw he aiai  fhsbntc\n",
      "58/58 [==============================] - 49s 851ms/step - loss: 2.8451 - accuracy: 0.1930\n",
      "Epoch 29/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.8379 - accuracy: 0.1944ad ee  lasero aeade re taadathn    tterfhly:rtaaet e idhnigyt-eemh e h t krawnef     e,,a.n whhw s, a\n",
      "58/58 [==============================] - 49s 845ms/step - loss: 2.8379 - accuracy: 0.1944\n",
      "Epoch 30/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.8361 - accuracy: 0.1919atoohoia:iier moicahhdhsinh.odo  dh  hs‘sada nieny,autoot l bt,riosrtpbynnwaw ot   o a n ncftmiepatio\n",
      "58/58 [==============================] - 48s 835ms/step - loss: 2.8361 - accuracy: 0.1919\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa4a0094dc0>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y, batch_size=batch_size, epochs=epochs, callbacks=[generate_text_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-portland",
   "metadata": {},
   "source": [
    "Finally, experiment with the trained model in the following cell to see how different seeds affect the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "beneficial-sheffield",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red-capo o  ssk smh d va d lsdttwsttiodseehuad,ahdih d ‘o euireo oslgerui s olioslerea ahoee bon!   teo.artirottorbust neah ya hclionwn ae‘e setanhwti,!i mynrrdt    lwr g l heateafe andatehnh rhwsh  ete he a\n"
     ]
    }
   ],
   "source": [
    "seed = \"red-cap\"\n",
    "num_chars_to_generate = 200\n",
    "\n",
    "generated_text = generate_text(model, seed, num_chars_to_generate, vocab)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-basket",
   "metadata": {},
   "source": [
    "## Part 3: Questions\n",
    "\n",
    "**Question 1:** This model performs *character-level* forecasting. Another approach would be to perform *word-level* forecasting, where the model takes a sequence of words and predicts the next word in the sequence. In the following cell, discuss the pros and cons of character-level vs. word-level text generation. What are 2 reasons why character-level forecasting might be preferable. What are two reasons why word-level forecasting might be preferable?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "verified-spirituality",
   "metadata": {},
   "source": [
    "Charater-level might be preferable because: 1. There is more data to train from. By taking a word appart by character, you are gaining more data to train from (essentially take the word count and multiply it by the average word length). 2. More accurate. They can learn words taht are more rarely used because of their better generation ability.\n",
    "\n",
    "Word-level generation might be preferable because: 1. It is less computationally expensive. Instead of taking each character, you are grouping them together, which will save the computational effort that would be expended if you were ar a character-level. 2. Might be better at predicting longer sentences. Can put together longer strings of words that make more sense than what a character-level forecasting would. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-residence",
   "metadata": {},
   "source": [
    "**Question 2:** The model you created was not given any specific instruction about English words, English grammar, or anything else related to the language other than the sequence of characters in the example text. What elements of proper English do you see emerging in the text generated after each training epoch? How many epochs does it take for these to appear? What does the model still struggle with?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "advisory-volume",
   "metadata": {},
   "source": [
    "The model seems to start grouping characters into groups separated by spaces after about 10 epochs and was able to group characters in lengths similar to the length of words one would find in a childs book. It also figured out a lot of words start with a consonant and are followed by vowels. It still struggles to form more than a few coherent words per epoch and has no clue how to use periods, apostrophes, commas, and other such non-alphabet characters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
